import os
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import gc
import psutil

# ğŸš¨ ç´§æ€¥å†…å­˜ä¼˜åŒ–é…ç½® - åœ¨æ‰€æœ‰æ“ä½œä¹‹å‰æ‰§è¡Œ
torch.cuda.empty_cache()
# é™åˆ¶PyTorchå†…å­˜åˆ†é…ç­–ç•¥ï¼Œå‡å°‘å†…å­˜ç¢ç‰‡
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'

from ..models.esrgan import LiteRealESRGAN, Discriminator
from ..models.losses import VGGLoss
from ..data.dataset import MemoryEfficientPairedDataset, create_data_loaders
from ..utils.metrics import calculate_psnr

class MemoryEfficientTrainer:
    def __init__(self, train_lr_dir, train_hr_dir, val_lr_dir, val_hr_dir, 
                 batch_size=2, lr=1e-4, device='cuda', max_cache_size=100,
                 gradient_accumulation_steps=4, mixed_precision=True, 
                 num_blocks=6, num_features=64):
        
        # ğŸš¨ é¢å¤–çš„å†…å­˜ä¼˜åŒ–è®¾ç½®
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        
        # å¼ºåˆ¶æ£€æŸ¥CUDAå¯ç”¨æ€§
        if not torch.cuda.is_available():
            raise RuntimeError("æ­¤ç‰ˆæœ¬ä»…æ”¯æŒGPUè®­ç»ƒï¼Œä½†æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼")
        
        # è®¾ç½®æ˜¾å­˜ä½¿ç”¨é™åˆ¶
        torch.cuda.set_per_process_memory_fraction(0.85)  # åªä½¿ç”¨85%æ˜¾å­˜
        
        self.device = torch.device('cuda')
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.mixed_precision = mixed_precision
        
        # ä¿å­˜æ¨¡å‹é…ç½®
        self.num_blocks = num_blocks
        self.num_features = num_features
        
        # æ£€æŸ¥GPUå†…å­˜
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"æ£€æµ‹åˆ°GPUæ˜¾å­˜: {gpu_memory:.1f}GB")
        print(f"ğŸ”§ å†…å­˜ä¼˜åŒ–é…ç½®å·²å¯ç”¨: max_split_size_mb=128, æ˜¾å­˜é™åˆ¶=85%")
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = torch.cuda.amp.GradScaler() if mixed_precision else None
        
        print(f"Using device: {self.device}")
        print(f"æ¨¡å‹é…ç½®: {num_blocks}å—, {num_features}ç‰¹å¾")
        print(f"å†…å­˜ä¼˜åŒ–è®¾ç½®: ç¼“å­˜å¤§å° {max_cache_size}, æ¢¯åº¦ç´¯ç§¯æ­¥æ•° {gradient_accumulation_steps}")
        print(f"æ··åˆç²¾åº¦è®­ç»ƒ: {'å¯ç”¨' if mixed_precision else 'ç¦ç”¨'}")
        
        # æ•°æ®åŠ è½½ - ä½¿ç”¨å†…å­˜é«˜æ•ˆçš„æ•°æ®åŠ è½½å™¨
        self.train_loader, self.val_loader = create_data_loaders(
            train_lr_dir, train_hr_dir, val_lr_dir, val_hr_dir,
            batch_size=batch_size, max_cache_size=max_cache_size, num_workers=0
        )
        
        # æ¨¡å‹åˆå§‹åŒ– - ä½¿ç”¨é…ç½®å‚æ•°
        self.generator = LiteRealESRGAN(num_blocks=num_blocks, num_features=num_features).to(self.device)
        self.discriminator = Discriminator().to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.g_optimizer = optim.Adam(self.generator.parameters(), lr=lr, betas=(0.9, 0.999))
        self.d_optimizer = optim.Adam(self.discriminator.parameters(), lr=lr, betas=(0.9, 0.999))
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.g_scheduler = optim.lr_scheduler.StepLR(self.g_optimizer, step_size=30, gamma=0.5)
        self.d_scheduler = optim.lr_scheduler.StepLR(self.d_optimizer, step_size=30, gamma=0.5)
        
        # æŸå¤±å‡½æ•°
        self.l1_loss = nn.L1Loss()
        self.vgg_loss = VGGLoss().to(self.device)
        self.gan_loss = nn.BCEWithLogitsLoss()
        
        # è®­ç»ƒçŠ¶æ€
        self.best_psnr = 0
        self.epoch = 0
        
        # å†…å­˜ç®¡ç†
        self.memory_cleanup_frequency = 20

    def get_system_memory_usage(self):
        """è·å–ç³»ç»Ÿå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        memory = psutil.virtual_memory()
        return {
            'total': memory.total / 1024**3,  # GB
            'available': memory.available / 1024**3,  # GB
            'used': memory.used / 1024**3,  # GB
            'percent': memory.percent
        }

    def clear_memory(self):
        """æ›´å½»åº•çš„å†…å­˜æ¸…ç†"""
        gc.collect()
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
    
    def train_epoch(self, epoch=None, total_epochs=None, progress_callback=None):
        """è®­ç»ƒä¸€ä¸ªepoch - GPUä¸“ç”¨ç‰ˆæœ¬"""
        self.generator.train()
        self.discriminator.train()
        
        total_g_loss = 0
        total_d_loss = 0
        
        for batch_idx, (lr_imgs, hr_imgs) in enumerate(self.train_loader):
            # å°†æ•°æ®ç§»åŠ¨åˆ°GPU
            lr_imgs = lr_imgs.to(self.device, non_blocking=True)
            hr_imgs = hr_imgs.to(self.device, non_blocking=True)
            batch_size = lr_imgs.size(0)
            
            # è®­ç»ƒåˆ¤åˆ«å™¨
            if batch_idx % self.gradient_accumulation_steps == 0:
                self.d_optimizer.zero_grad()
            
            try:
                with torch.cuda.amp.autocast(enabled=self.mixed_precision):
                    # ç”Ÿæˆè¶…åˆ†è¾¨ç‡å›¾åƒ
                    sr_imgs = self.generator(lr_imgs)
                    
                    # çœŸå®å›¾åƒ
                    real_labels = torch.ones(batch_size, 1, device=self.device)
                    real_pred = self.discriminator(hr_imgs)
                    d_real_loss = self.gan_loss(real_pred, real_labels)
                    
                    # ç”Ÿæˆå›¾åƒ
                    fake_labels = torch.zeros(batch_size, 1, device=self.device)
                    fake_pred = self.discriminator(sr_imgs.detach())
                    d_fake_loss = self.gan_loss(fake_pred, fake_labels)
                    
                    d_loss = (d_real_loss + d_fake_loss) / (2 * self.gradient_accumulation_steps)
                
                if self.scaler and self.mixed_precision:
                    self.scaler.scale(d_loss).backward()
                else:
                    d_loss.backward()
                
                if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                    if self.scaler and self.mixed_precision:
                        self.scaler.step(self.d_optimizer)
                        self.scaler.update()
                    else:
                        self.d_optimizer.step()
                    self.d_optimizer.zero_grad()
                
                # è®­ç»ƒç”Ÿæˆå™¨
                if batch_idx % self.gradient_accumulation_steps == 0:
                    self.g_optimizer.zero_grad()
                
                with torch.cuda.amp.autocast(enabled=self.mixed_precision):
                    # é‡æ–°ç”Ÿæˆ
                    sr_imgs = self.generator(lr_imgs)
                    
                    # L1æŸå¤±
                    l1_loss = self.l1_loss(sr_imgs, hr_imgs)
                    
                    # æ„ŸçŸ¥æŸå¤±
                    perceptual_loss = self.vgg_loss(sr_imgs, hr_imgs)
                    
                    # GANæŸå¤±
                    fake_pred = self.discriminator(sr_imgs)
                    gan_loss = self.gan_loss(fake_pred, real_labels)
                    
                    # æ€»ç”Ÿæˆå™¨æŸå¤±
                    g_loss = (l1_loss + 0.1 * perceptual_loss + 0.01 * gan_loss) / self.gradient_accumulation_steps
                
                if self.scaler and self.mixed_precision:
                    self.scaler.scale(g_loss).backward()
                else:
                    g_loss.backward()
                
                if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
                    if self.scaler and self.mixed_precision:
                        self.scaler.step(self.g_optimizer)
                        self.scaler.update()
                    else:
                        self.g_optimizer.step()
                    self.g_optimizer.zero_grad()
                
                total_g_loss += g_loss.item() * self.gradient_accumulation_steps
                total_d_loss += d_loss.item() * self.gradient_accumulation_steps
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"âŒ GPUå†…å­˜ä¸è¶³: {str(e)}")
                    self.clear_memory()
                    raise e
                else:
                    raise e
            
            # å®šæœŸæ¸…ç†å†…å­˜
            if batch_idx % self.memory_cleanup_frequency == 0:
                self.clear_memory()
            
            # åˆ é™¤ä¸éœ€è¦çš„å¼ é‡
            try:
                del lr_imgs, hr_imgs, sr_imgs, real_pred, fake_pred
                del real_labels, fake_labels, l1_loss, perceptual_loss, gan_loss
                del d_real_loss, d_fake_loss
            except:
                pass
            
            # è¿›åº¦å›è°ƒ
            if progress_callback and batch_idx % 10 == 0:
                progress_info = {
                    'epoch': epoch or self.epoch,
                    'total_epochs': total_epochs or 1,
                    'batch': batch_idx + 1,
                    'total_batches': len(self.train_loader),
                    'g_loss': total_g_loss/(batch_idx+1) if batch_idx > 0 else 0,
                    'd_loss': total_d_loss/(batch_idx+1) if batch_idx > 0 else 0,
                    'progress': (batch_idx + 1) / len(self.train_loader)
                }
                progress_callback(progress_info)
            
            # è¯¦ç»†è¾“å‡º
            if batch_idx % 10 == 0:
                current_g_loss = total_g_loss/(batch_idx+1) if batch_idx > 0 else 0
                current_d_loss = total_d_loss/(batch_idx+1) if batch_idx > 0 else 0
                
                print(f"[Epoch {epoch or self.epoch}/{total_epochs or 1}] "
                      f"Batch {batch_idx+1}/{len(self.train_loader)} [GPU] - "
                      f"G_Loss: {current_g_loss:.6f}, D_Loss: {current_d_loss:.6f}")
                
                # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                memory_used = torch.cuda.memory_allocated() / 1024**3
                memory_cached = torch.cuda.memory_reserved() / 1024**3
                print(f"GPUå†…å­˜: ä½¿ç”¨ {memory_used:.2f}GB, ç¼“å­˜ {memory_cached:.2f}GB")
        
        # æœ€ç»ˆæ¸…ç†
        self.clear_memory()
        
        if len(self.train_loader) > 0:
            return total_g_loss / len(self.train_loader), total_d_loss / len(self.train_loader)
        else:
            return 0, 0

    def get_memory_info(self):
        """è·å–GPUå†…å­˜ä¿¡æ¯"""
        return {
            'gpu_allocated': torch.cuda.memory_allocated() / 1024**3,
            'gpu_cached': torch.cuda.memory_reserved() / 1024**3,
            'gpu_total': torch.cuda.get_device_properties(0).total_memory / 1024**3
        }
    
    def validate(self):
        """éªŒè¯æ¨¡å‹"""
        self.generator.eval()
        total_psnr = 0
        
        with torch.no_grad():
            for batch_idx, (lr_imgs, hr_imgs) in enumerate(self.val_loader):
                lr_imgs = lr_imgs.to(self.device, non_blocking=True)
                hr_imgs = hr_imgs.to(self.device, non_blocking=True)
                
                sr_imgs = self.generator(lr_imgs)
                
                # è®¡ç®—PSNR (è½¬æ¢åˆ°[0,1]èŒƒå›´)
                sr_imgs_norm = (sr_imgs + 1) / 2
                hr_imgs_norm = (hr_imgs + 1) / 2
                
                psnr = calculate_psnr(sr_imgs_norm, hr_imgs_norm)
                total_psnr += psnr.item()
                
                # åˆ é™¤å¼ é‡é‡Šæ”¾å†…å­˜
                del lr_imgs, hr_imgs, sr_imgs, sr_imgs_norm, hr_imgs_norm
                
                # å®šæœŸæ¸…ç†å†…å­˜
                if batch_idx % 20 == 0:
                    self.clear_memory()
        
        # æ¸…ç†éªŒè¯åçš„å†…å­˜
        self.clear_memory()
        
        avg_psnr = total_psnr / len(self.val_loader)
        return avg_psnr
    
    def save_checkpoint(self, filepath, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.epoch,
            'generator_state_dict': self.generator.state_dict(),
            'discriminator_state_dict': self.discriminator.state_dict(),
            'g_optimizer_state_dict': self.g_optimizer.state_dict(),
            'd_optimizer_state_dict': self.d_optimizer.state_dict(),
            'g_scheduler_state_dict': self.g_scheduler.state_dict(),
            'd_scheduler_state_dict': self.d_scheduler.state_dict(),
            'best_psnr': self.best_psnr
        }
        torch.save(checkpoint, filepath)
        
        if is_best:
            best_path = filepath.replace('.pth', '_best.pth')
            torch.save(checkpoint, best_path)
            print(f"Best model saved to {best_path}")
    
    def load_checkpoint(self, filepath):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        try:
            print(f"æ­£åœ¨åŠ è½½æ£€æŸ¥ç‚¹: {filepath}")
            checkpoint = torch.load(filepath, map_location=self.device, weights_only=False)
            
            # æ£€æŸ¥æ£€æŸ¥ç‚¹æ ¼å¼
            required_keys = ['generator_state_dict', 'discriminator_state_dict']
            
            for key in required_keys:
                if key not in checkpoint:
                    raise KeyError(f"æ£€æŸ¥ç‚¹ç¼ºå°‘å¿…è¦çš„é”®: {key}")
            
            # åŠ è½½ç”Ÿæˆå™¨çŠ¶æ€
            try:
                self.generator.load_state_dict(checkpoint['generator_state_dict'])
                print("âœ… ç”Ÿæˆå™¨çŠ¶æ€åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âŒ ç”Ÿæˆå™¨çŠ¶æ€åŠ è½½å¤±è´¥: {e}")
                raise
            
            # åŠ è½½åˆ¤åˆ«å™¨çŠ¶æ€
            try:
                self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
                print("âœ… åˆ¤åˆ«å™¨çŠ¶æ€åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âŒ åˆ¤åˆ«å™¨çŠ¶æ€åŠ è½½å¤±è´¥: {e}")
                raise
            
            # æ£€æŸ¥æ¨¡å‹ç»“æ„æ˜¯å¦åŒ¹é…
            model_structure_changed = self._check_model_structure_change(checkpoint)
            
            # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆä»…åœ¨æ¨¡å‹ç»“æ„æœªå˜åŒ–æ—¶ï¼‰
            if not model_structure_changed and 'g_optimizer_state_dict' in checkpoint and 'd_optimizer_state_dict' in checkpoint:
                try:
                    self.g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])
                    self.d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])
                    print("âœ… ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½æˆåŠŸ")
                except Exception as e:
                    print(f"âš ï¸  ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤çŠ¶æ€: {e}")
                    print("ğŸ”„ ä¼˜åŒ–å™¨å°†ä»é»˜è®¤çŠ¶æ€å¼€å§‹")
            else:
                if model_structure_changed:
                    print("âš ï¸  æ£€æµ‹åˆ°æ¨¡å‹ç»“æ„å˜åŒ–ï¼Œè·³è¿‡ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½")
                    print("ğŸ”„ ä¼˜åŒ–å™¨å°†ä»é»˜è®¤çŠ¶æ€å¼€å§‹ï¼Œè¿™æ˜¯æ­£å¸¸çš„")
                else:
                    print("âš ï¸  æ£€æŸ¥ç‚¹ä¸­ç¼ºå°‘ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œå°†ä½¿ç”¨é»˜è®¤çŠ¶æ€")
            
            # åŠ è½½å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€ï¼ˆä»…åœ¨æ¨¡å‹ç»“æ„æœªå˜åŒ–æ—¶ï¼‰
            if not model_structure_changed and 'g_scheduler_state_dict' in checkpoint and 'd_scheduler_state_dict' in checkpoint:
                try:
                    self.g_scheduler.load_state_dict(checkpoint['g_scheduler_state_dict'])
                    self.d_scheduler.load_state_dict(checkpoint['d_scheduler_state_dict'])
                    print("âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€åŠ è½½æˆåŠŸ")
                except Exception as e:
                    print(f"âš ï¸  å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤çŠ¶æ€: {e}")
            else:
                if model_structure_changed:
                    print("âš ï¸  æ¨¡å‹ç»“æ„å˜åŒ–ï¼Œå­¦ä¹ ç‡è°ƒåº¦å™¨å°†ä»é»˜è®¤çŠ¶æ€å¼€å§‹")
            
            # åŠ è½½è®­ç»ƒä¿¡æ¯
            self.epoch = checkpoint.get('epoch', 0)
            self.best_psnr = checkpoint.get('best_psnr', 0.0)
            
            print(f"âœ… æ£€æŸ¥ç‚¹åŠ è½½å®Œæˆ")
            print(f"   - èµ·å§‹epoch: {self.epoch}")
            print(f"   - æœ€ä½³PSNR: {self.best_psnr:.2f} dB")
            
            if model_structure_changed:
                print("ğŸ”„ ç”±äºæ¨¡å‹ç»“æ„å˜åŒ–ï¼Œä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨å·²é‡ç½®")
                print("ğŸ’¡ å»ºè®®é™ä½å­¦ä¹ ç‡ä»¥è·å¾—æ›´ç¨³å®šçš„è®­ç»ƒ")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
            print(f"   æ–‡ä»¶è·¯å¾„: {filepath}")
            import traceback
            traceback.print_exc()
            return False
    
    def _check_model_structure_change(self, checkpoint):
        """æ£€æŸ¥æ¨¡å‹ç»“æ„æ˜¯å¦å‘ç”Ÿå˜åŒ–"""
        try:
            # æ£€æŸ¥ç”Ÿæˆå™¨å‚æ•°å½¢çŠ¶
            current_state = self.generator.state_dict()
            checkpoint_state = checkpoint['generator_state_dict']
            
            for key in current_state.keys():
                if key in checkpoint_state:
                    if current_state[key].shape != checkpoint_state[key].shape:
                        print(f"ğŸ” æ£€æµ‹åˆ°å‚æ•°å½¢çŠ¶å˜åŒ–: {key}")
                        print(f"   æ£€æŸ¥ç‚¹: {checkpoint_state[key].shape}")
                        print(f"   å½“å‰æ¨¡å‹: {current_state[key].shape}")
                        return True
                else:
                    print(f"ğŸ” æ£€æµ‹åˆ°æ–°å¢å‚æ•°: {key}")
                    return True
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å‚æ•°è¢«åˆ é™¤
            for key in checkpoint_state.keys():
                if key not in current_state:
                    print(f"ğŸ” æ£€æµ‹åˆ°åˆ é™¤çš„å‚æ•°: {key}")
                    return True
            
            return False
            
        except Exception as e:
            print(f"âš ï¸  æ¨¡å‹ç»“æ„æ£€æŸ¥å¤±è´¥: {e}")
            # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œä¸ºäº†å®‰å…¨èµ·è§ï¼Œå‡è®¾ç»“æ„å‘ç”Ÿäº†å˜åŒ–
            return True
    
    def train(self, num_epochs, save_dir='checkpoints', save_frequency=5):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        os.makedirs(save_dir, exist_ok=True)
        
        print(f"å¼€å§‹GPUè®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepoch...")
        print(f"è®­ç»ƒæ•°æ®é›†å¤§å°: {len(self.train_loader.dataset)}")
        print(f"éªŒè¯æ•°æ®é›†å¤§å°: {len(self.val_loader.dataset)}")
        
        # è®°å½•èµ·å§‹epochï¼Œé¿å…è¦†ç›–ä»æ£€æŸ¥ç‚¹åŠ è½½çš„epoch
        start_epoch = self.epoch
        
        for epoch in range(start_epoch, start_epoch + num_epochs):
            # ä¸è¦é‡æ–°è®¾ç½®self.epochï¼Œä¿æŒä»æ£€æŸ¥ç‚¹åŠ è½½çš„å€¼
            start_time = time.time()
            
            print(f"\n=== Epoch {epoch+1}/{start_epoch + num_epochs} ===")
            
            # æ¸…ç†æ•°æ®é›†ç¼“å­˜
            if hasattr(self.train_loader.dataset, 'clear_cache'):
                if epoch % 10 == 0:  # æ¯10ä¸ªepochæ¸…ç†ä¸€æ¬¡ç¼“å­˜
                    self.train_loader.dataset.clear_cache()
                    self.val_loader.dataset.clear_cache()
                    print("æ•°æ®é›†ç¼“å­˜å·²æ¸…ç†")
            
            # è®­ç»ƒ
            print("å¼€å§‹è®­ç»ƒ...")
            g_loss, d_loss = self.train_epoch()
            
            # éªŒè¯
            print("å¼€å§‹éªŒè¯...")
            val_psnr = self.validate()
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.g_scheduler.step()
            self.d_scheduler.step()
            
            epoch_time = time.time() - start_time
            
            print(f"\nEpoch {epoch+1} å®Œæˆ:")
            print(f"G_Loss: {g_loss:.4f}, D_Loss: {d_loss:.4f}")
            print(f"Val PSNR: {val_psnr:.2f} dB")
            print(f"å­¦ä¹ ç‡: G={self.g_scheduler.get_last_lr()[0]:.6f}, D={self.d_scheduler.get_last_lr()[0]:.6f}")
            print(f"ç”¨æ—¶: {epoch_time:.2f}s")
            
            # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
            memory_used = torch.cuda.memory_allocated() / 1024**3
            memory_cached = torch.cuda.memory_reserved() / 1024**3
            print(f"GPUå†…å­˜: ä½¿ç”¨ {memory_used:.2f}GB, ç¼“å­˜ {memory_cached:.2f}GB")
            
            print("-" * 60)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            is_best = val_psnr > self.best_psnr
            if is_best:
                self.best_psnr = val_psnr
                print(f"ğŸ‰ æ–°çš„æœ€ä½³PSNR: {self.best_psnr:.2f} dB")
            
            if (epoch + 1) % save_frequency == 0 or is_best:
                checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth')
                # æ›´æ–°self.epochä¸ºå½“å‰epochï¼Œä»¥ä¾¿æ­£ç¡®ä¿å­˜åˆ°æ£€æŸ¥ç‚¹
                self.epoch = epoch
                self.save_checkpoint(checkpoint_path, is_best)
                print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        
        # è®­ç»ƒç»“æŸåæ›´æ–°æœ€ç»ˆçš„epoch
        self.epoch = start_epoch + num_epochs - 1
        print(f"\nğŸŠ è®­ç»ƒå®Œæˆ! æœ€ä½³PSNR: {self.best_psnr:.2f} dB")


# ä¿æŒå‘åå…¼å®¹æ€§
Trainer = MemoryEfficientTrainer


def main():
    # é…ç½®å‚æ•°
    config = {
        'train_lr_dir': 'data/train/lr',  # ä½åˆ†è¾¨ç‡è®­ç»ƒå›¾åƒç›®å½•
        'train_hr_dir': 'data/train/hr',  # é«˜åˆ†è¾¨ç‡è®­ç»ƒå›¾åƒç›®å½•
        'val_lr_dir': 'data/val/lr',      # ä½åˆ†è¾¨ç‡éªŒè¯å›¾åƒç›®å½•
        'val_hr_dir': 'data/val/hr',      # é«˜åˆ†è¾¨ç‡éªŒè¯å›¾åƒç›®å½•
        'batch_size': 4,                  # GPUè®­ç»ƒå¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡
        'learning_rate': 1e-4,
        'num_epochs': 100,
        'device': 'cuda',
        'max_cache_size': 500,            # GPUè®­ç»ƒå¯ä»¥ä½¿ç”¨æ›´å¤§çš„ç¼“å­˜
        'gradient_accumulation_steps': 2   # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    }
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    for dir_path in [config['train_lr_dir'], config['train_hr_dir'], 
                     config['val_lr_dir'], config['val_hr_dir']]:
        if not os.path.exists(dir_path):
            print(f"Warning: Directory {dir_path} does not exist!")
            print("Please create the data directories and add your training images.")
            return
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MemoryEfficientTrainer(
        train_lr_dir=config['train_lr_dir'],
        train_hr_dir=config['train_hr_dir'],
        val_lr_dir=config['val_lr_dir'],
        val_hr_dir=config['val_hr_dir'],
        batch_size=config['batch_size'],
        lr=config['learning_rate'],
        device=config['device'],
        max_cache_size=config['max_cache_size'],
        gradient_accumulation_steps=config['gradient_accumulation_steps']
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(num_epochs=config['num_epochs'])

if __name__ == "__main__":
    main()