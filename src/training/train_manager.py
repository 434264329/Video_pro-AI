import os
import time
import json
import threading
from datetime import datetime
import torch
import matplotlib.pyplot as plt
import gc

# ğŸš¨ ç´§æ€¥å†…å­˜ä¼˜åŒ–é…ç½® - åœ¨æ‰€æœ‰æ“ä½œä¹‹å‰æ‰§è¡Œ
torch.cuda.empty_cache()
# é™åˆ¶PyTorchå†…å­˜åˆ†é…ç­–ç•¥ï¼Œå‡å°‘å†…å­˜ç¢ç‰‡
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'

from .trainer import MemoryEfficientTrainer
from ..utils.metrics import calculate_psnr

class MemoryOptimizedTrainingManager:
    """å†…å­˜ä¼˜åŒ–çš„è®­ç»ƒç®¡ç†å™¨ - è´Ÿè´£è®­ç»ƒæµç¨‹æ§åˆ¶å’Œç›‘æ§"""
    
    def __init__(self, config):
        # ğŸš¨ åˆå§‹åŒ–æ—¶ç«‹å³æ‰§è¡Œå†…å­˜ä¼˜åŒ–
        torch.cuda.empty_cache()
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            # è®¾ç½®æ˜¾å­˜ä½¿ç”¨é™åˆ¶
            torch.cuda.set_per_process_memory_fraction(0.85)
            print("ğŸ”§ è®­ç»ƒç®¡ç†å™¨å†…å­˜ä¼˜åŒ–é…ç½®å·²å¯ç”¨")
        
        self.config = config
        self.trainer = None
        self.is_training = False
        self.training_thread = None
        self.progress_callback = None
        self.is_incremental = False  # æ–°å¢ï¼šæ ‡è®°æ˜¯å¦ä¸ºå¢é‡è®­ç»ƒ
        self.checkpoint_path = None  # æ–°å¢ï¼šé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
        self.callbacks = {
            'on_epoch_start': [],
            'on_epoch_end': [],
            'on_training_start': [],
            'on_training_end': [],
            'on_progress_update': [],
            'on_memory_status': [],
            'on_error': []
        }
        
        # è®­ç»ƒå†å²
        self.training_history = {
            'epochs': [],
            'train_g_loss': [],
            'train_d_loss': [],
            'val_psnr': [],
            'learning_rates': [],
            'times': [],
            'memory_usage': []
        }
        
        # å†…å­˜ç›‘æ§
        self.memory_monitor_enabled = True
        
    def add_callback(self, event, callback):
        """æ·»åŠ å›è°ƒå‡½æ•°"""
        if event in self.callbacks:
            self.callbacks[event].append(callback)
    
    def _trigger_callback(self, event, *args, **kwargs):
        """è§¦å‘å›è°ƒå‡½æ•°"""
        for callback in self.callbacks.get(event, []):
            try:
                callback(*args, **kwargs)
            except Exception as e:
                print(f"å›è°ƒå‡½æ•°æ‰§è¡Œé”™è¯¯: {e}")
    
    def get_memory_info(self):
        """è·å–å†…å­˜ä½¿ç”¨ä¿¡æ¯"""
        memory_info = {}
        
        if torch.cuda.is_available():
            memory_info['gpu_allocated'] = torch.cuda.memory_allocated() / 1024**3  # GB
            memory_info['gpu_cached'] = torch.cuda.memory_reserved() / 1024**3  # GB
            memory_info['gpu_max_allocated'] = torch.cuda.max_memory_allocated() / 1024**3  # GB
        
        # å¯ä»¥æ·»åŠ ç³»ç»Ÿå†…å­˜ç›‘æ§
        try:
            import psutil
            memory_info['system_memory'] = psutil.virtual_memory().percent
        except ImportError:
            memory_info['system_memory'] = 0
        
        return memory_info
    
    def prepare_training(self, checkpoint_path=None):
        """å‡†å¤‡è®­ç»ƒ"""
        try:
            print(f"ğŸ”§ æ­£åœ¨å‡†å¤‡è®­ç»ƒç¯å¢ƒ...")
            
            # éªŒè¯æ•°æ®ç›®å½•
            required_dirs = [
                self.config['data']['train_lr_dir'],
                self.config['data']['train_hr_dir'],
                self.config['data']['val_lr_dir'],
                self.config['data']['val_hr_dir']
            ]
            
            missing_dirs = []
            for dir_path in required_dirs:
                if not os.path.exists(dir_path):
                    missing_dirs.append(dir_path)
            
            if missing_dirs:
                error_msg = f"ä»¥ä¸‹æ•°æ®ç›®å½•ä¸å­˜åœ¨: {', '.join(missing_dirs)}"
                print(f"âŒ {error_msg}")
                self._trigger_callback('on_error', error_msg)
                return False
            
            print("âœ… æ•°æ®ç›®å½•éªŒè¯é€šè¿‡")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(self.config['paths']['save_dir'], exist_ok=True)
            os.makedirs(self.config['paths']['log_dir'], exist_ok=True)
            print("âœ… è¾“å‡ºç›®å½•åˆ›å»ºå®Œæˆ")
            
            # å†…å­˜ä¼˜åŒ–é…ç½®
            memory_config = self.config.get('memory', {})
            max_cache_size = memory_config.get('max_cache_size', 500)
            gradient_accumulation_steps = memory_config.get('gradient_accumulation_steps', 1)
            
            print(f"ğŸ“Š å†…å­˜é…ç½®: ç¼“å­˜å¤§å°={max_cache_size}, æ¢¯åº¦ç´¯ç§¯æ­¥æ•°={gradient_accumulation_steps}")
            
            # åˆ›å»ºå†…å­˜ä¼˜åŒ–è®­ç»ƒå™¨
            print("ğŸš€ æ­£åœ¨åˆ›å»ºè®­ç»ƒå™¨...")
            self.trainer = MemoryEfficientTrainer(
                train_lr_dir=self.config['data']['train_lr_dir'],
                train_hr_dir=self.config['data']['train_hr_dir'],
                val_lr_dir=self.config['data']['val_lr_dir'],
                val_hr_dir=self.config['data']['val_hr_dir'],
                batch_size=self.config['training']['batch_size'],
                lr=self.config['training']['learning_rate'],
                device=self.config['training']['device'],
                max_cache_size=max_cache_size,
                gradient_accumulation_steps=gradient_accumulation_steps,
                num_blocks=self.config['model']['num_blocks'],
                num_features=self.config['model']['num_features']
            )
            print("âœ… è®­ç»ƒå™¨åˆ›å»ºæˆåŠŸ")
            
            # å¦‚æœæä¾›äº†æ£€æŸ¥ç‚¹è·¯å¾„ï¼ŒåŠ è½½é¢„è®­ç»ƒæ¨¡å‹
            if checkpoint_path and os.path.exists(checkpoint_path):
                print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ£€æŸ¥ç‚¹: {os.path.basename(checkpoint_path)}")
                
                # ä½¿ç”¨æ”¹è¿›çš„åŠ è½½æ–¹æ³•
                if self.trainer.load_checkpoint(checkpoint_path):
                    self.is_incremental = True
                    self.checkpoint_path = checkpoint_path
                    print(f"âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
                    print(f"ğŸ“ˆ å°†ä»ç¬¬ {self.trainer.epoch + 1} ä¸ªepochå¼€å§‹å¢é‡è®­ç»ƒ")
                else:
                    error_msg = f"æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {checkpoint_path}"
                    print(f"âŒ {error_msg}")
                    self._trigger_callback('on_error', error_msg)
                    return False
            
            print("ğŸ‰ è®­ç»ƒå‡†å¤‡å®Œæˆ")
            return True
            
        except Exception as e:
            error_msg = f"è®­ç»ƒå‡†å¤‡å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            import traceback
            traceback.print_exc()
            self._trigger_callback('on_error', error_msg)
            return False
    
    def start_training(self, progress_callback=None, checkpoint_path=None):
        """å¼€å§‹è®­ç»ƒ"""
        if self.is_training:
            return False
            
        if not self.trainer:
            if not self.prepare_training(checkpoint_path):
                return False
        
        # ä¿å­˜è¿›åº¦å›è°ƒå‡½æ•°
        self.progress_callback = progress_callback
        
        self.is_training = True
        self.training_thread = threading.Thread(target=self._training_loop)
        self.training_thread.daemon = True
        self.training_thread.start()
        
        return True
    
    def start_incremental_training(self, checkpoint_path, progress_callback=None):
        """å¼€å§‹å¢é‡è®­ç»ƒ"""
        if not os.path.exists(checkpoint_path):
            self._trigger_callback('on_error', f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return False
        
        # ä¸ºå¢é‡è®­ç»ƒè°ƒæ•´é…ç½®
        original_lr = self.config['training']['learning_rate']
        self.config['training']['learning_rate'] = original_lr * 0.1
        print(f"å¢é‡è®­ç»ƒå­¦ä¹ ç‡è°ƒæ•´ä¸º: {self.config['training']['learning_rate']}")
        print(f"å¢é‡è®­ç»ƒå°†è¿›è¡Œ {self.config['training']['num_epochs']} ä¸ªepoch")
        
        return self.start_training(progress_callback, checkpoint_path)
    
    def stop_training(self):
        """åœæ­¢è®­ç»ƒ"""
        self.is_training = False
        if self.training_thread:
            self.training_thread.join(timeout=5)
    
    def _training_loop(self):
        """è®­ç»ƒå¾ªç¯"""
        epoch = 0
        num_epochs = 0
        memory_after = {}
        
        try:
            self._trigger_callback('on_training_start')
            
            num_epochs = self.config['training']['num_epochs']
            save_frequency = self.config['training'].get('save_frequency', 5)
            
            # å¦‚æœæ˜¯å¢é‡è®­ç»ƒï¼Œä»å·²æœ‰çš„epochå¼€å§‹
            start_epoch = self.trainer.epoch if self.is_incremental else 0
            
            for epoch in range(start_epoch, start_epoch + num_epochs):
                if not self.is_training:
                    break
                    
                self._trigger_callback('on_epoch_start', epoch)
                
                start_time = time.time()
                
                # è·å–è®­ç»ƒå‰çš„å†…å­˜çŠ¶æ€
                memory_before = self.get_memory_info()
                
                # åˆ›å»ºè¿›åº¦å›è°ƒå‡½æ•°ï¼Œç”¨äºæ›´æ–°æ§åˆ¶å°è®­ç»ƒå™¨çš„batchä¿¡æ¯
                def batch_progress_callback(progress_info):
                    # æ›´æ–°æ§åˆ¶å°è®­ç»ƒå™¨çš„ç»Ÿè®¡ä¿¡æ¯
                    if hasattr(self, 'console_trainer'):
                        self.console_trainer.training_stats['current_batch'] = progress_info['batch']
                        self.console_trainer.training_stats['total_batches'] = progress_info['total_batches']
                        self.console_trainer.training_stats['g_loss'] = progress_info['g_loss']
                        self.console_trainer.training_stats['d_loss'] = progress_info['d_loss']
                        self.console_trainer.training_stats['total_batches_processed'] += 1
                
                # è®­ç»ƒä¸€ä¸ªepochï¼Œä¼ é€’è¿›åº¦å›è°ƒ
                g_loss, d_loss = self.trainer.train_epoch(
                    epoch=epoch + 1, 
                    total_epochs=start_epoch + num_epochs,
                    progress_callback=batch_progress_callback
                )
                
                # éªŒè¯
                val_psnr = self.trainer.validate()
                
                # è·å–è®­ç»ƒåçš„å†…å­˜çŠ¶æ€
                memory_after = self.get_memory_info()
                
                epoch_time = time.time() - start_time
                
                # è®°å½•å†å²
                self.training_history['epochs'].append(epoch + 1)
                self.training_history['train_g_loss'].append(g_loss)
                self.training_history['train_d_loss'].append(d_loss)
                self.training_history['val_psnr'].append(val_psnr)
                self.training_history['times'].append(epoch_time)
                self.training_history['memory_usage'].append(memory_after)
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                # ä¿å­˜æ£€æŸ¥ç‚¹
                is_best = val_psnr > self.trainer.best_psnr
                if is_best:
                    self.trainer.best_psnr = val_psnr
                
                if (epoch + 1) % save_frequency == 0 or is_best:
                    checkpoint_name = f'checkpoint_epoch_{epoch+1}.pth'
                    if self.is_incremental:
                        checkpoint_name = f'incremental_checkpoint_epoch_{epoch+1}.pth'
                    
                    checkpoint_path = os.path.join(
                        self.config['paths']['save_dir'], 
                        checkpoint_name
                    )
                    # ç¡®ä¿ä¿å­˜æ—¶epochæ˜¯æ­£ç¡®çš„
                    self.trainer.epoch = epoch + 1
                    self.trainer.save_checkpoint(checkpoint_path, is_best)
                
                # è§¦å‘epochç»“æŸå›è°ƒ
                epoch_info = {
                    'epoch': epoch + 1,
                    'total_epochs': start_epoch + num_epochs,
                    'g_loss': g_loss,
                    'd_loss': d_loss,
                    'val_psnr': val_psnr,
                    'time': epoch_time,
                    'is_best': is_best,
                    'memory_before': memory_before,
                    'memory_after': memory_after,
                    'is_incremental': self.is_incremental
                }
                
                self._trigger_callback('on_epoch_end', epoch_info)
                
                # å†…å­˜æ¸…ç†
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
                
                # æ˜¾ç¤ºè®­ç»ƒä¿¡æ¯
                training_type = "å¢é‡è®­ç»ƒ" if self.is_incremental else "è®­ç»ƒ"
                print(f"{training_type} Epoch {epoch+1}/{start_epoch + num_epochs} - G Loss: {g_loss:.4f}, D Loss: {d_loss:.4f}, Val PSNR: {val_psnr:.2f}")
                
                # è§¦å‘è¿›åº¦æ›´æ–°å›è°ƒ
                if num_epochs > 0:
                    progress_percent = (epoch - start_epoch + 1) / num_epochs * 100
                    self._trigger_callback('on_progress_update', progress_percent)
                
                # è§¦å‘å†…å­˜çŠ¶æ€å›è°ƒ
                if self.memory_monitor_enabled:
                    self._trigger_callback('on_memory_status', memory_after)
                
                # å®šæœŸæ¸…ç†å†…å­˜
                if (epoch + 1) % 10 == 0:
                    gc.collect()
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
            
            # è®­ç»ƒå®Œæˆåçš„å›è°ƒ
            self._trigger_callback('on_training_end', self.training_history)
            
        except Exception as e:
            self._trigger_callback('on_error', f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            print(f"è®­ç»ƒé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self.is_training = False
    
    def get_training_status(self):
        """è·å–è®­ç»ƒçŠ¶æ€"""
        # è®¡ç®—å½“å‰å®é™…çš„epoch
        current_epoch = 0
        total_epochs = 0
        
        if self.trainer and self.config:
            if self.is_incremental:
                # å¢é‡è®­ç»ƒï¼šå½“å‰epoch = æ£€æŸ¥ç‚¹epoch + å·²å®Œæˆçš„æ–°epochæ•°
                start_epoch = self.trainer.epoch
                completed_new_epochs = len(self.training_history['epochs'])
                current_epoch = start_epoch + completed_new_epochs
                total_epochs = start_epoch + self.config['training']['num_epochs']
            else:
                # æ™®é€šè®­ç»ƒ
                current_epoch = len(self.training_history['epochs'])
                total_epochs = self.config['training']['num_epochs']
        
        status = {
            'is_training': self.is_training,
            'current_epoch': current_epoch,
            'total_epochs': total_epochs,
            'best_psnr': self.trainer.best_psnr if self.trainer else 0,
            'history': self.training_history,
            'is_incremental': self.is_incremental,
            'start_epoch': self.trainer.epoch if self.trainer and self.is_incremental else 0
        }
        
        # æ·»åŠ å†…å­˜ä¿¡æ¯
        if self.memory_monitor_enabled:
            status['memory_info'] = self.get_memory_info()
        
        return status
    
    def save_training_log(self, filepath):
        """ä¿å­˜è®­ç»ƒæ—¥å¿—"""
        log_data = {
            'config': self.config,
            'history': self.training_history,
            'timestamp': datetime.now().isoformat(),
            'final_best_psnr': self.trainer.best_psnr if self.trainer else 0,
            'memory_optimization': True
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, indent=2, ensure_ascii=False)
    
    def plot_training_curves(self, save_path=None):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        if not self.training_history['epochs']:
            return None
            
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        epochs = self.training_history['epochs']
        
        # ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨æŸå¤±
        ax1.plot(epochs, self.training_history['train_g_loss'], 'b-', label='Generator Loss')
        ax1.plot(epochs, self.training_history['train_d_loss'], 'r-', label='Discriminator Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title('Training Losses')
        ax1.legend()
        ax1.grid(True)
        
        # éªŒè¯PSNR
        ax2.plot(epochs, self.training_history['val_psnr'], 'g-', label='Validation PSNR')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('PSNR (dB)')
        ax2.set_title('Validation PSNR')
        ax2.legend()
        ax2.grid(True)
        
        # è®­ç»ƒæ—¶é—´
        ax3.plot(epochs, self.training_history['times'], 'm-', label='Epoch Time')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Time (seconds)')
        ax3.set_title('Training Time per Epoch')
        ax3.legend()
        ax3.grid(True)
        
        # å†…å­˜ä½¿ç”¨æƒ…å†µ
        if self.training_history['memory_usage']:
            gpu_memory = [mem.get('gpu_allocated', 0) for mem in self.training_history['memory_usage']]
            ax4.plot(epochs, gpu_memory, 'c-', label='GPU Memory (GB)')
            ax4.set_xlabel('Epoch')
            ax4.set_ylabel('Memory (GB)')
            ax4.set_title('GPU Memory Usage')
            ax4.legend()
            ax4.grid(True)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")
        
        return fig


# ä¿æŒå‘åå…¼å®¹æ€§
TrainingManager = MemoryOptimizedTrainingManager